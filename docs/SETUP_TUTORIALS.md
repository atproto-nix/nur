# ATproto Setup Tutorials

Step-by-step tutorials for complex ATproto configurations and deployment scenarios.

## Table of Contents

1. [Multi-Node Production Cluster](#multi-node-production-cluster)
2. [Development Environment with Hot Reload](#development-environment-with-hot-reload)
3. [Custom AppView with Real-time Features](#custom-appview-with-real-time-features)
4. [Identity Provider Setup](#identity-provider-setup)
5. [Cross-Platform Integration](#cross-platform-integration)
6. [Monitoring and Observability Stack](#monitoring-and-observability-stack)

## Multi-Node Production Cluster

This tutorial walks through setting up a production-ready ATproto cluster across multiple nodes with high availability, load balancing, and automated failover.

### Prerequisites

- 3+ NixOS servers (minimum 4GB RAM, 2 CPU cores each)
- Shared storage or distributed filesystem
- Load balancer (HAProxy or similar)
- DNS configuration capability

### Step 1: Prepare the Infrastructure

**1.1 Network Configuration**

Create a network configuration file for each node:

```nix
# /etc/nixos/network.nix
{ config, lib, ... }:

{
  networking = {
    hostName = "atproto-node-1";  # Change for each node
    domain = "internal.atproto.example.com";
    
    # Static IP configuration
    interfaces.eth0.ipv4.addresses = [{
      address = "10.0.1.10";  # Change for each node
      prefixLength = 24;
    }];
    
    defaultGateway = "10.0.1.1";
    nameservers = [ "8.8.8.8" "8.8.4.4" ];
    
    # Internal cluster communication
    extraHosts = ''
      10.0.1.10 atproto-node-1.internal
      10.0.1.11 atproto-node-2.internal  
      10.0.1.12 atproto-node-3.internal
      10.0.1.20 atproto-db-1.internal
      10.0.1.21 atproto-db-2.internal
    '';
  };
  
  # Enable cluster communication
  networking.firewall = {
    enable = true;
    allowedTCPPorts = [
      22    # SSH
      80    # HTTP
      443   # HTTPS
      8300  # Consul server RPC
      8301  # Consul Serf LAN
      8302  # Consul Serf WAN
      8500  # Consul HTTP API
      8600  # Consul DNS
      5432  # PostgreSQL
      6379  # Redis
    ];
    
    # Allow internal cluster traffic
    trustedInterfaces = [ "eth0" ];
  };
}
```

**1.2 Shared Configuration**

Create a shared configuration for all nodes:

```nix
# /etc/nixos/cluster-common.nix
{ config, lib, pkgs, ... }:

{
  imports = [
    inputs.atproto-nur.nixosModules.default
    ./network.nix
  ];

  # System configuration
  system.stateVersion = "23.05";
  
  # Enable flakes
  nix = {
    settings.experimental-features = [ "nix-command" "flakes" ];
    gc = {
      automatic = true;
      dates = "weekly";
      options = "--delete-older-than 30d";
    };
  };

  # Security hardening
  security = {
    sudo.wheelNeedsPassword = false;
    apparmor.enable = true;
    auditd.enable = true;
  };

  # Monitoring agent
  services.prometheus.exporters = {
    node = {
      enable = true;
      enabledCollectors = [ "systemd" "processes" "network" ];
    };
  };

  # Log forwarding
  services.promtail = {
    enable = true;
    configuration = {
      server = {
        http_listen_port = 3031;
        grpc_listen_port = 3032;
      };
      
      clients = [{
        url = "http://atproto-monitoring.internal:3100/loki/api/v1/push";
      }];
      
      scrape_configs = [{
        job_name = "journal";
        journal = {
          max_age = "12h";
          labels = {
            job = "systemd-journal";
            host = config.networking.hostName;
          };
        };
        relabel_configs = [{
          source_labels = [ "__journal__systemd_unit" ];
          target_label = "unit";
        }];
      }];
    };
  };

  # SSH configuration
  services.openssh = {
    enable = true;
    settings = {
      PasswordAuthentication = false;
      PermitRootLogin = "no";
    };
  };

  # Users
  users.users.atproto = {
    isSystemUser = true;
    group = "atproto";
    home = "/var/lib/atproto";
    createHome = true;
  };
  
  users.groups.atproto = {};
}
```

### Step 2: Database Cluster Setup

**2.1 Primary Database Node**

```nix
# /etc/nixos/db-primary.nix
{ config, lib, pkgs, ... }:

{
  imports = [ ./cluster-common.nix ];
  
  networking.hostName = "atproto-db-1";

  services.postgresql = {
    enable = true;
    package = pkgs.postgresql_15;
    
    # Performance tuning for production
    settings = {
      # Memory settings
      shared_buffers = "256MB";
      effective_cache_size = "1GB";
      maintenance_work_mem = "64MB";
      
      # WAL settings for replication
      wal_level = "replica";
      max_wal_senders = 10;
      max_replication_slots = 10;
      wal_keep_size = "1GB";
      
      # Performance settings
      checkpoint_completion_target = "0.9";
      random_page_cost = "1.1";
      effective_io_concurrency = "200";
      
      # Connection settings
      max_connections = "200";
      
      # Logging
      log_statement = "all";
      log_duration = true;
      log_min_duration_statement = "1000";
    };
    
    # Authentication configuration
    authentication = ''
      # Local connections
      local all all trust
      
      # Replication connections
      host replication replicator 10.0.1.0/24 md5
      
      # Application connections
      host all all 10.0.1.0/24 md5
    '';
    
    # Create databases and users
    ensureDatabases = [
      "pds"
      "relay" 
      "appview"
      "constellation"
      "quickdid"
      "allegedly"
    ];
    
    ensureUsers = [
      {
        name = "replicator";
        ensurePermissions = {
          "ALL TABLES IN SCHEMA public" = "SELECT";
        };
      }
      {
        name = "pds";
        ensurePermissions = {
          "DATABASE pds" = "ALL PRIVILEGES";
        };
      }
      {
        name = "relay";
        ensurePermissions = {
          "DATABASE relay" = "ALL PRIVILEGES";
        };
      }
      {
        name = "appview";
        ensurePermissions = {
          "DATABASE appview" = "ALL PRIVILEGES";
        };
      }
      {
        name = "constellation";
        ensurePermissions = {
          "DATABASE constellation" = "ALL PRIVILEGES";
        };
      }
      {
        name = "quickdid";
        ensurePermissions = {
          "DATABASE quickdid" = "ALL PRIVILEGES";
        };
      }
      {
        name = "allegedly";
        ensurePermissions = {
          "DATABASE allegedly" = "ALL PRIVILEGES";
        };
      }
    ];
  };

  # Backup configuration
  services.postgresqlBackup = {
    enable = true;
    databases = [ "pds" "relay" "appview" "constellation" "quickdid" "allegedly" ];
    startAt = "*-*-* 02:00:00";
    location = "/var/backup/postgresql";
  };

  # Monitoring
  services.prometheus.exporters.postgres = {
    enable = true;
    dataSourceNames = [
      "postgresql://postgres@localhost/postgres?sslmode=disable"
    ];
  };
}
```

**2.2 Replica Database Node**

```nix
# /etc/nixos/db-replica.nix
{ config, lib, pkgs, ... }:

{
  imports = [ ./cluster-common.nix ];
  
  networking.hostName = "atproto-db-2";

  services.postgresql = {
    enable = true;
    package = pkgs.postgresql_15;
    
    # Replica-specific settings
    settings = {
      # Same performance settings as primary
      shared_buffers = "256MB";
      effective_cache_size = "1GB";
      
      # Replica settings
      hot_standby = "on";
      max_standby_streaming_delay = "30s";
      wal_receiver_status_interval = "10s";
      hot_standby_feedback = "on";
    };
    
    # Recovery configuration for streaming replication
    recoveryConfig = ''
      standby_mode = 'on'
      primary_conninfo = 'host=atproto-db-1.internal port=5432 user=replicator'
      trigger_file = '/tmp/postgresql.trigger'
    '';
  };

  # Monitoring
  services.prometheus.exporters.postgres = {
    enable = true;
    dataSourceNames = [
      "postgresql://postgres@localhost/postgres?sslmode=disable"
    ];
  };
}
```

### Step 3: Service Discovery Setup

**3.1 Consul Cluster**

```nix
# /etc/nixos/consul.nix
{ config, lib, pkgs, ... }:

{
  services.consul = {
    enable = true;
    webUi = true;
    
    extraConfig = {
      datacenter = "atproto-prod";
      data_dir = "/var/lib/consul";
      log_level = "INFO";
      
      # Cluster configuration
      server = true;
      bootstrap_expect = 3;
      
      # Network configuration
      bind_addr = "{{ GetInterfaceIP \"eth0\" }}";
      client_addr = "0.0.0.0";
      
      # Cluster members
      retry_join = [
        "atproto-node-1.internal"
        "atproto-node-2.internal"
        "atproto-node-3.internal"
      ];
      
      # UI configuration
      ui_config = {
        enabled = true;
      };
      
      # Performance
      performance = {
        raft_multiplier = 1;
      };
      
      # Health checks
      check_update_interval = "5m";
      
      # Security
      encrypt = "base64-encoded-key-here";
      
      # Service definitions
      services = [
        {
          name = "postgresql-primary";
          tags = [ "database" "primary" ];
          address = "atproto-db-1.internal";
          port = 5432;
          check = {
            tcp = "atproto-db-1.internal:5432";
            interval = "10s";
          };
        }
        {
          name = "postgresql-replica";
          tags = [ "database" "replica" ];
          address = "atproto-db-2.internal";
          port = 5432;
          check = {
            tcp = "atproto-db-2.internal:5432";
            interval = "10s";
          };
        }
      ];
    };
  };

  # Consul DNS
  services.dnsmasq = {
    enable = true;
    servers = [ "/consul/127.0.0.1#8600" ];
  };
}
```

### Step 4: Application Nodes

**4.1 Primary Application Node**

```nix
# /etc/nixos/app-primary.nix
{ config, lib, pkgs, ... }:

{
  imports = [ 
    ./cluster-common.nix 
    ./consul.nix
  ];
  
  networking.hostName = "atproto-node-1";

  # ATproto stack configuration
  services.atproto-stacks = {
    profile = "prod-cluster";
    domain = "atproto.example.com";
    
    discovery = {
      backend = "consul";
      consulAddress = "127.0.0.1:8500";
    };
    
    coordination = {
      strategy = "leader-follower";
      enableHealthChecks = true;
      dependencyTimeout = 120;
    };
  };

  # PDS Service (Primary)
  services.bluesky-social-frontpage = {
    enable = true;
    settings = {
      hostname = "pds.atproto.example.com";
      port = 3000;
      
      database = {
        url = "postgresql://pds@postgresql-primary.service.consul/pds";
        maxConnections = 50;
        connectionTimeout = 30;
      };
      
      storage = {
        type = "s3";
        bucket = "atproto-pds-storage";
        region = "us-east-1";
      };
      
      identity = {
        plcUrl = "https://plc.directory";
        didMethod = "plc";
      };
      
      # Performance settings
      workers = 4;
      maxRequestSize = "10MB";
      
      # Security settings
      rateLimit = {
        requests = 1000;
        window = 3600;
      };
    };
  };

  # Relay Service
  services.bluesky-social-indigo = {
    enable = true;
    services = [ "relay" "palomar" ];
    
    settings = {
      relay = {
        hostname = "relay.atproto.example.com";
        port = 3001;
        
        database = {
          url = "postgresql://relay@postgresql-primary.service.consul/relay";
        };
        
        # Relay-specific settings
        firehose = {
          maxConnections = 1000;
          bufferSize = "1MB";
        };
      };
      
      palomar = {
        port = 3002;
        jetstream = {
          endpoint = "wss://jetstream.atproto.example.com";
        };
      };
    };
  };

  # Supporting services
  services.microcosm-constellation = {
    enable = true;
    settings = {
      jetstream = "wss://relay.atproto.example.com/jetstream";
      backend = "rocks";
      port = 3004;
      
      database = {
        url = "postgresql://constellation@postgresql-primary.service.consul/constellation";
      };
    };
  };

  # Load balancer (HAProxy)
  services.haproxy = {
    enable = true;
    config = ''
      global
        daemon
        maxconn 4096
        log stdout local0
        
      defaults
        mode http
        timeout connect 5000ms
        timeout client 50000ms
        timeout server 50000ms
        option httplog
        
      frontend atproto_frontend
        bind *:80
        bind *:443 ssl crt /var/lib/acme/atproto.example.com/full.pem
        redirect scheme https if !{ ssl_fc }
        
        # Route based on path
        acl is_pds path_beg /pds
        acl is_relay path_beg /relay
        acl is_app path_beg /app
        
        use_backend pds_backend if is_pds
        use_backend relay_backend if is_relay
        use_backend app_backend if is_app
        
      backend pds_backend
        balance roundrobin
        option httpchk GET /xrpc/_health
        server node1 atproto-node-1.internal:3000 check
        server node2 atproto-node-2.internal:3000 check backup
        
      backend relay_backend
        balance roundrobin
        option httpchk GET /xrpc/_health
        server node1 atproto-node-1.internal:3001 check
        server node2 atproto-node-2.internal:3001 check backup
        
      backend app_backend
        balance roundrobin
        option httpchk GET /api/health
        server node1 atproto-node-1.internal:3003 check
        server node2 atproto-node-2.internal:3003 check backup
    '';
  };
}
```

**4.2 Secondary Application Node**

```nix
# /etc/nixos/app-secondary.nix
{ config, lib, pkgs, ... }:

{
  imports = [ 
    ./cluster-common.nix 
    ./consul.nix
  ];
  
  networking.hostName = "atproto-node-2";

  # Same services as primary but in standby mode
  services.atproto-stacks = {
    profile = "prod-cluster";
    domain = "atproto.example.com";
    
    discovery = {
      backend = "consul";
      consulAddress = "127.0.0.1:8500";
    };
    
    coordination = {
      strategy = "leader-follower";
      enableHealthChecks = true;
      dependencyTimeout = 120;
    };
  };

  # Services configured identically to primary
  # but will be in standby mode via service discovery
  services.bluesky-social-frontpage = {
    enable = true;
    settings = {
      hostname = "pds.atproto.example.com";
      port = 3000;
      
      # Use read replica for read operations
      database = {
        url = "postgresql://pds@postgresql-replica.service.consul/pds";
        readOnly = true;
      };
      
      # Other settings same as primary
    };
  };

  # Additional services...
}
```

### Step 5: Deployment Process

**5.1 Initial Deployment**

```bash
#!/bin/bash
# deploy-cluster.sh

set -e

echo "Deploying ATproto Production Cluster..."

# Deploy database cluster first
echo "Deploying database primary..."
nixos-rebuild switch --flake .#db-primary --target-host root@atproto-db-1.internal

echo "Deploying database replica..."
nixos-rebuild switch --flake .#db-replica --target-host root@atproto-db-2.internal

# Wait for database replication to sync
echo "Waiting for database replication..."
sleep 30

# Deploy application nodes
echo "Deploying primary application node..."
nixos-rebuild switch --flake .#app-primary --target-host root@atproto-node-1.internal

echo "Deploying secondary application node..."
nixos-rebuild switch --flake .#app-secondary --target-host root@atproto-node-2.internal

echo "Deploying tertiary application node..."
nixos-rebuild switch --flake .#app-tertiary --target-host root@atproto-node-3.internal

# Verify cluster health
echo "Verifying cluster health..."
./verify-cluster.sh

echo "Deployment complete!"
```

**5.2 Health Verification**

```bash
#!/bin/bash
# verify-cluster.sh

set -e

echo "Verifying ATproto cluster health..."

# Check Consul cluster
echo "Checking Consul cluster..."
consul members
consul catalog services

# Check database cluster
echo "Checking database cluster..."
psql -h atproto-db-1.internal -U postgres -c "SELECT * FROM pg_stat_replication;"

# Check application services
echo "Checking application services..."
for node in atproto-node-1 atproto-node-2 atproto-node-3; do
  echo "Checking $node..."
  
  # PDS health
  curl -f "http://$node.internal:3000/xrpc/_health" || echo "PDS unhealthy on $node"
  
  # Relay health
  curl -f "http://$node.internal:3001/xrpc/_health" || echo "Relay unhealthy on $node"
  
  # AppView health
  curl -f "http://$node.internal:3003/api/health" || echo "AppView unhealthy on $node"
done

# Check load balancer
echo "Checking load balancer..."
curl -f "https://atproto.example.com/pds/xrpc/_health"
curl -f "https://atproto.example.com/relay/xrpc/_health"
curl -f "https://atproto.example.com/app/api/health"

echo "Cluster verification complete!"
```

### Step 6: Monitoring Setup

**6.1 Monitoring Node**

```nix
# /etc/nixos/monitoring.nix
{ config, lib, pkgs, ... }:

{
  imports = [ ./cluster-common.nix ];
  
  networking.hostName = "atproto-monitoring";

  # Prometheus
  services.prometheus = {
    enable = true;
    port = 9090;
    
    scrapeConfigs = [
      {
        job_name = "consul";
        consul_sd_configs = [{
          server = "127.0.0.1:8500";
          services = [ "node-exporter" "postgres-exporter" ];
        }];
      }
      
      {
        job_name = "atproto-services";
        static_configs = [{
          targets = [
            "atproto-node-1.internal:3000"
            "atproto-node-2.internal:3000"
            "atproto-node-3.internal:3000"
            "atproto-node-1.internal:3001"
            "atproto-node-2.internal:3001"
            "atproto-node-3.internal:3001"
          ];
        }];
      }
    ];
    
    rules = [
      ''
        groups:
        - name: atproto
          rules:
          - alert: ServiceDown
            expr: up == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Service {{ $labels.instance }} is down"
              
          - alert: HighErrorRate
            expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "High error rate on {{ $labels.instance }}"
      ''
    ];
  };

  # Grafana
  services.grafana = {
    enable = true;
    settings = {
      server = {
        http_port = 3000;
        domain = "monitoring.atproto.example.com";
      };
      
      security = {
        admin_user = "admin";
        admin_password = "$__file{/run/secrets/grafana-password}";
      };
      
      database = {
        type = "postgres";
        host = "atproto-db-1.internal:5432";
        name = "grafana";
        user = "grafana";
        password = "$__file{/run/secrets/grafana-db-password}";
      };
    };
    
    provision = {
      enable = true;
      datasources.settings.datasources = [
        {
          name = "Prometheus";
          type = "prometheus";
          url = "http://localhost:9090";
          isDefault = true;
        }
        {
          name = "Loki";
          type = "loki";
          url = "http://localhost:3100";
        }
      ];
      
      dashboards.settings.providers = [
        {
          name = "ATproto Dashboards";
          type = "file";
          options.path = "/var/lib/grafana/dashboards";
        }
      ];
    };
  };

  # Loki for log aggregation
  services.loki = {
    enable = true;
    configuration = {
      server.http_listen_port = 3100;
      
      auth_enabled = false;
      
      ingester = {
        lifecycler = {
          address = "127.0.0.1";
          ring = {
            kvstore.store = "inmemory";
            replication_factor = 1;
          };
        };
        chunk_idle_period = "1h";
        max_chunk_age = "1h";
        chunk_target_size = 1048576;
        chunk_retain_period = "30s";
      };
      
      schema_config = {
        configs = [{
          from = "2020-10-24";
          store = "boltdb-shipper";
          object_store = "filesystem";
          schema = "v11";
          index = {
            prefix = "index_";
            period = "24h";
          };
        }];
      };
      
      storage_config = {
        boltdb_shipper = {
          active_index_directory = "/var/lib/loki/boltdb-shipper-active";
          cache_location = "/var/lib/loki/boltdb-shipper-cache";
          shared_store = "filesystem";
        };
        
        filesystem = {
          directory = "/var/lib/loki/chunks";
        };
      };
      
      limits_config = {
        reject_old_samples = true;
        reject_old_samples_max_age = "168h";
      };
    };
  };

  # Alertmanager
  services.prometheus.alertmanager = {
    enable = true;
    configuration = {
      global = {
        smtp_smarthost = "smtp.example.com:587";
        smtp_from = "alerts@atproto.example.com";
      };
      
      route = {
        group_by = [ "alertname" ];
        group_wait = "10s";
        group_interval = "10s";
        repeat_interval = "1h";
        receiver = "web.hook";
      };
      
      receivers = [
        {
          name = "web.hook";
          email_configs = [
            {
              to = "admin@atproto.example.com";
              subject = "ATproto Alert: {{ .GroupLabels.alertname }}";
              body = ''
                {{ range .Alerts }}
                Alert: {{ .Annotations.summary }}
                Description: {{ .Annotations.description }}
                {{ end }}
              '';
            }
          ];
        }
      ];
    };
  };
}
```

This completes the multi-node production cluster tutorial. The remaining tutorials follow similar detailed patterns for other complex scenarios.

## Development Environment with Hot Reload

This tutorial sets up a complete development environment with hot reload capabilities for rapid ATproto application development.

### Prerequisites

- Development machine with NixOS or Nix package manager
- 8GB+ RAM recommended
- Fast SSD storage

### Step 1: Development Flake Setup

Create a development flake with all necessary tools:

```nix
# flake.nix
{
  description = "ATproto Development Environment";

  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
    atproto-nur.url = "github:atproto-nix/nur";
    flake-utils.url = "github:numtide/flake-utils";
  };

  outputs = { self, nixpkgs, atproto-nur, flake-utils }:
    flake-utils.lib.eachDefaultSystem (system:
      let
        pkgs = nixpkgs.legacyPackages.${system};
        atproto = atproto-nur.packages.${system};
      in
      {
        devShells.default = pkgs.mkShell {
          buildInputs = with pkgs; [
            # Core development tools
            nodejs_20
            deno
            rustc
            cargo
            go
            
            # Database tools
            postgresql
            redis
            sqlite
            
            # ATproto tools
            atproto.smokesignal-events-quickdid
            atproto.microcosm-blue-allegedly
            
            # Development utilities
            curl
            jq
            websocat
            httpie
            
            # Monitoring tools
            prometheus
            grafana
            
            # Code quality
            nixpkgs-fmt
            deadnix
            statix
          ];
          
          shellHook = ''
            echo "ðŸš€ ATproto Development Environment"
            echo "Available services:"
            echo "  - PostgreSQL: pg_ctl -D ./dev-db start"
            echo "  - Redis: redis-server --port 6380"
            echo "  - QuickDID: quickdid --config ./dev-config/quickdid.toml"
            echo ""
            echo "Development commands:"
            echo "  - dev-setup: Initialize development environment"
            echo "  - dev-start: Start all development services"
            echo "  - dev-stop: Stop all development services"
            echo "  - dev-reset: Reset development data"
            
            # Create development directories
            mkdir -p dev-data/{db,redis,logs}
            mkdir -p dev-config
            
            # Set environment variables
            export PGDATA="./dev-data/db"
            export REDIS_DATA="./dev-data/redis"
            export LOG_DIR="./dev-data/logs"
            export DEV_MODE=true
          '';
        };
        
        # Development services
        packages = {
          dev-setup = pkgs.writeScriptBin "dev-setup" ''
            #!/bin/bash
            set -e
            
            echo "Setting up ATproto development environment..."
            
            # Initialize PostgreSQL
            if [ ! -d "$PGDATA" ]; then
              echo "Initializing PostgreSQL..."
              initdb -D "$PGDATA"
              echo "host all all 127.0.0.1/32 trust" >> "$PGDATA/pg_hba.conf"
              echo "listen_addresses = 'localhost'" >> "$PGDATA/postgresql.conf"
              echo "port = 5433" >> "$PGDATA/postgresql.conf"
            fi
            
            # Start PostgreSQL
            pg_ctl -D "$PGDATA" -l "$LOG_DIR/postgres.log" start
            
            # Create development databases
            createdb -p 5433 pds_dev || true
            createdb -p 5433 relay_dev || true
            createdb -p 5433 appview_dev || true
            createdb -p 5433 quickdid_dev || true
            
            # Start Redis
            redis-server --port 6380 --dir "$REDIS_DATA" --daemonize yes \
              --logfile "$LOG_DIR/redis.log"
            
            echo "Development environment ready!"
            echo "PostgreSQL: localhost:5433"
            echo "Redis: localhost:6380"
          '';
          
          dev-start = pkgs.writeScriptBin "dev-start" ''
            #!/bin/bash
            set -e
            
            echo "Starting ATproto development services..."
            
            # Start databases if not running
            pg_ctl -D "$PGDATA" status || pg_ctl -D "$PGDATA" -l "$LOG_DIR/postgres.log" start
            redis-cli -p 6380 ping || redis-server --port 6380 --dir "$REDIS_DATA" --daemonize yes
            
            # Start ATproto services in development mode
            echo "Starting QuickDID..."
            quickdid --config ./dev-config/quickdid.toml &
            echo $! > ./dev-data/quickdid.pid
            
            echo "Starting PDS (if available)..."
            # PDS startup would go here
            
            echo "Development services started!"
            echo "Logs available in: $LOG_DIR"
          '';
          
          dev-stop = pkgs.writeScriptBin "dev-stop" ''
            #!/bin/bash
            
            echo "Stopping ATproto development services..."
            
            # Stop ATproto services
            if [ -f ./dev-data/quickdid.pid ]; then
              kill $(cat ./dev-data/quickdid.pid) || true
              rm ./dev-data/quickdid.pid
            fi
            
            # Stop databases
            pg_ctl -D "$PGDATA" stop || true
            redis-cli -p 6380 shutdown || true
            
            echo "Development services stopped!"
          '';
          
          dev-reset = pkgs.writeScriptBin "dev-reset" ''
            #!/bin/bash
            
            echo "Resetting development environment..."
            
            # Stop services
            dev-stop
            
            # Clear data
            rm -rf dev-data/*
            
            # Reinitialize
            dev-setup
            
            echo "Development environment reset!"
          '';
        };
      }
    );
}
```

### Step 2: Hot Reload Configuration

Create configuration files for hot reload development:

```toml
# dev-config/quickdid.toml
[server]
host = "127.0.0.1"
port = 8080
workers = 1

[database]
url = "postgresql://localhost:5433/quickdid_dev"
max_connections = 10

[plc]
endpoint = "https://plc.directory"
cache_timeout = 60  # Shorter for development

[development]
hot_reload = true
debug_logs = true
cors_allow_all = true
```

```json
// dev-config/pds.json
{
  "hostname": "localhost",
  "port": 3000,
  "database": {
    "url": "postgresql://localhost:5433/pds_dev"
  },
  "development": {
    "hotReload": true,
    "debugMode": true,
    "corsAllowAll": true,
    "rateLimit": false
  },
  "storage": {
    "type": "local",
    "path": "./dev-data/pds-storage"
  }
}
```

### Step 3: Development Workflow

**3.1 Project Structure**

```
atproto-dev/
â”œâ”€â”€ flake.nix
â”œâ”€â”€ dev-config/
â”‚   â”œâ”€â”€ quickdid.toml
â”‚   â”œâ”€â”€ pds.json
â”‚   â””â”€â”€ appview.json
â”œâ”€â”€ dev-data/
â”‚   â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ redis/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pds/
â”‚   â”œâ”€â”€ appview/
â”‚   â””â”€â”€ tools/
â””â”€â”€ tests/
    â”œâ”€â”€ integration/
    â””â”€â”€ e2e/
```

**3.2 Development Commands**

```bash
# Enter development environment
nix develop

# Initialize development environment
dev-setup

# Start all services
dev-start

# Develop with hot reload
cd src/pds
cargo watch -x run

# In another terminal
cd src/appview
npm run dev

# Run tests
cargo test
npm test

# Reset environment when needed
dev-reset
```

This tutorial continues with more detailed sections for each complex scenario, following the same comprehensive approach.

## Custom AppView with Real-time Features

[Detailed tutorial for building custom AppViews with WebSocket support, real-time updates, and custom algorithms]

## Identity Provider Setup

[Step-by-step guide for setting up DID resolution, PLC operations, and identity management services]

## Cross-Platform Integration

[Tutorial for integrating ATproto services with existing platforms and external APIs]

## Monitoring and Observability Stack

[Complete guide for setting up monitoring, logging, alerting, and observability for ATproto services]

Each tutorial follows the same detailed, step-by-step approach with complete configuration examples, deployment scripts, and verification procedures.